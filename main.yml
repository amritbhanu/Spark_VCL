---
- hosts: all
  vars:
    user1: aagrawa8
  user: "{{user1}}"

  tasks:
   - name: cloning repo
     sudo: false
     command: git clone https://github.com/amritbhanu/Spark_VCL.git

   - name: create spark dir
     sudo: false
     file: path=/home/{{user1}}/spark state=directory mode=0755

   - name: download spark
     sudo: false
     get_url: url=http://ftp.wayne.edu/apache//spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz dest=/home/{{user1}}/spark/

   - name: Unarchive Spark download
     sudo: false
     unarchive: src=/home/{{user1}}/spark/spark-1.6.0-bin-hadoop2.6.tgz dest=/home/{{user1}}/spark copy=no

   - name: Move spark dir to spark_latest
     sudo: false
     command: mv /home/{{user1}}/spark/spark-1.6.0-bin-hadoop2.6 /home/{{user1}}/spark/spark_latest

   - name: download hadoop
     sudo: false
     get_url: url=http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz dest=/home/{{user1}}/

   - name: Unarchive hadoop download
     sudo: false
     unarchive: src=/home/{{user1}}/hadoop-2.6.0.tar.gz dest=/home/{{user1}}/ copy=no

   - name: Move hadoop-2 dir to hadoop
     sudo: false
     command: mv /home/{{user1}}/hadoop-2.6.0 /home/{{user1}}/hadoop

   - name: Changing ownership
     sudo: false
     command: chown -R {{user1}} /home/{{user1}}/hadoop

   - name: writing to bashrc
     lineinfile: dest=/home/{{user1}}/.bashrc
                regexp=''
                insertafter=EOF
                line='export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64\nexport HADOOP_INSTALL=/home/aagrawa8/hadoop\nexport PATH=$PATH:$HADOOP_INSTALL/bin\nexport PATH=$PATH:$HADOOP_INSTALL/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_HOME=$HADOOP_INSTALL\nexport HADOOP_HDFS_HOME=$HADOOP_INSTALL\nexport YARN_HOME=$HADOOP_INSTALL\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native'

   - name: update bashrc
     sudo: false
     shell: source /home/{{user1}}/.bashrc
     args:
        executable: /bin/bash

   - name: Create directroy namenode
     sudo: false
     command: mkdir -pv /home/{{user1}}/hadoop/data/namenode

   - name: Create directroy logs
     sudo: false
     command: mkdir -pv /home/{{user1}}/hadoop/logs


   - name: Create directroy datanode
     sudo: false
     command: mkdir -pv /home/{{user1}}/hadoop/data/datanode

   - name: writing to hadoop-env.sh
     lineinfile: dest=/home/{{user1}}/hadoop/etc/hadoop/hadoop-env.sh
                regexp=''
                insertafter=EOF
                line='export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64'

   - name: add lines in hdfs-site.xml
     lineinfile: dest=/home/{{user1}}/hadoop/etc/hadoop/hdfs-site.xml
                regexp='^<configuration>'
                insertafter='<configuration>'
                line='<property><name>dfs.namenode.name.dir</name><value>file:///home/{{user1}}/hadoop/data/namenode</value><description>NameNode directory for namespace and transaction logs storage.</description></property><property><name>dfs.replication</name><value>3</value></property><property><name>dfs.permissions</name><value>false</value></property><property><name>dfs.datanode.use.datanode.hostname</name><value>false</value></property><property><name>dfs.namenode.datanode.registration.ip-hostname-check</name><value>false</value></property><property><name>dfs.namenode.http-address</name><value>{{inventory_hostname}}:50070</value><description>Your NameNode hostname for http access.</description></property><property><name>dfs.namenode.secondary.http-address</name><value>{{inventory_hostname}}:50090</value><description>Your Secondary NameNode hostname for http access.</description></property>\n\n<property><name>dfs.datanode.data.dir</name><value>file:///home/{{user1}}/hadoop/data/datanode</value><description>DataNode directory</description></property><property><name>dfs.replication</name><value>3</value></property><property><name>dfs.permissions</name><value>false</value></property><property><name>dfs.datanode.use.datanode.hostname</name><value>false</value></property><property><name>dfs.namenode.http-address</name><value>{{inventory_hostname}}:50070</value><description>Your NameNode hostname for http access.</description></property><property><name>dfs.namenode.secondary.http-address</name><value>{{inventory_hostname}}:50090</value><description>Your Secondary NameNode hostname for http access.</description></property>'
                state=present

   - name: add lines in core-site.xml
     lineinfile: dest=/home/{{user1}}/hadoop/etc/hadoop/core-site.xml
                regexp='^<configuration>'
                insertafter='<configuration>'
                line='<property><name>fs.defaultFS</name><value>hdfs://{{inventory_hostname}}/</value><description>NameNode URI</description></property>'
                state=present

   - name: Move mapred template to mapred-site
     sudo: false
     command: mv /home/{{user1}}/hadoop/etc/hadoop/mapred-site.xml.template /home/{{user1}}/hadoop/etc/hadoop/mapred-site.xml	

   - name: add lines in mapred-site.xml
     lineinfile: dest=/home/{{user1}}/hadoop/etc/hadoop/mapred-site.xml
                regexp='^<configuration>'
                insertafter='<configuration>'
                line='<property><name>mapreduce.framework.name</name><value>yarn</value></property>'
                state=present

   - name: add lines in yarn-site.xml
     lineinfile: dest=/home/{{user1}}/hadoop/etc/hadoop/yarn-site.xml
                regexp='^<configuration>'
                insertafter='<configuration>'
                line='<property><name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value></property><property><name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name><value>org.apache.hadoop.mapred.ShuffleHandler</value></property><property><name>yarn.resourcemanager.resource-tracker.address</name><value>{{inventory_hostname}}:8025</value></property><property><name>yarn.resourcemanager.scheduler.address</name><value>{{inventory_hostname}}:8030</value></property><property><name>yarn.resourcemanager.address</name><value>{{inventory_hostname}}:8050</value></property>'
                state=present

   - name: hdfs namenode format
     sudo: false
     shell: hdfs namenode -format

